# OWASP Top 10 for Large Language Model(LLM) Applications   - from Linkedin Learning

Note: released on Aug 2023

### 1. Prompt Injection attack

When attackers manipulate the output of an LLM by creating a harmful command that causes the model to ignore the rules or guidelines set by the creator
- to gain unauthorized access to information or to manipulate data in the llm
- harmful commands
- manipulated external inputs - plugins, extensions, addons, additional modules interact with langauge model
- exfiltration, data theft

### 2. Insecure Output handling


### 3. LLM build and deployment process


### 4. Model Denial of Service


### 5. Supply Chain Vulnerability


### 6. Sensitive Information disclosure


### 7. Insecure plugin design


### 8. Excessive agency


### 9. Over reliance


### 10. Model theft


